{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPggCX0c2NeVntdcxegaTmw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohadi21/Pr-diction-des-donn-es-MNIST-avec-uniquement-100-labels/blob/main/SGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZSbOasfj1UU"
      },
      "source": [
        "# ***Importation de bibliothèques***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAYhDENLg5_y",
        "outputId": "b53a1a92-aae5-4185-b8df-badee7b3f960"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Lambda\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "\n",
        "%matplotlib inline\n",
        "print('done importing libraries')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done importing libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFFb5Dd_k3Km"
      },
      "source": [
        "# ***Les dimensions des images cibles***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYjmVZ673WzH",
        "outputId": "3e0fb52a-a55f-4c3f-cad7-6cd868439f14"
      },
      "source": [
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "z_dim = 100 #random noise input for generator \n",
        "num_classes = 10  #no. of classes to predict for semi supervised algo\n",
        "\n",
        "\n",
        "print('done defining variables')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done defining variables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRD-UaJXlRUp"
      },
      "source": [
        "# **Le jeu de données MNIST **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQF_8jQB3nXE",
        "outputId": "2904bc1f-48ac-42de-eec8-aa3833d30a47"
      },
      "source": [
        "def load_mnist_data():\n",
        "    \n",
        "    return mnist.load_data()\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, num_labeled):\n",
        "        self.num_labeled = num_labeled\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = load_mnist_data()\n",
        "        \n",
        "        def preprocess_images(x):\n",
        "            x = (x.astype(np.float32) - 127.5) / 127.5\n",
        "            print(x.shape)\n",
        "            x = np.expand_dims(x, axis=3)\n",
        "            print(x.shape)\n",
        "            \n",
        "            return x\n",
        "        def preprocess_labels(y):\n",
        "            return y.reshape(-1, 1)\n",
        "        \n",
        "        self.x_train = preprocess_images(self.x_train)\n",
        "        self.x_test = preprocess_images(self.x_test)\n",
        "        \n",
        "        self.y_train = preprocess_labels(self.y_train)\n",
        "        self.y_test = preprocess_labels(self.y_test)\n",
        "        \n",
        "    def batch_labeled(self, batch_size):\n",
        "        idx = np.random.randint(0, self.num_labeled, size=batch_size)\n",
        "        images = self.x_train[idx]\n",
        "        labels = self.y_train[idx]\n",
        "\n",
        "        return images, labels\n",
        "    \n",
        "    def batch_unlabeled(self, batch_size):\n",
        "        idx = np.random.randint(self.num_labeled, self.x_train.shape[0], batch_size)\n",
        "        \n",
        "        return self.x_train[idx]\n",
        "    \n",
        "    def training_set(self):\n",
        "        x_train = self.x_train[:self.num_labeled]\n",
        "        y_train = self.y_train[:self.num_labeled]\n",
        "        \n",
        "        return x_train, y_train\n",
        "    \n",
        "    def test_set(self):\n",
        "        \n",
        "        return self.x_test, self.y_test\n",
        "\n",
        "print('done defining the dataset class')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done defining the dataset class\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkFJ97L436f2",
        "outputId": "df9aa9f5-ae0d-4bea-c664-531977cd08a9"
      },
      "source": [
        "num_labeled = 100\n",
        "dataset = Dataset(num_labeled)\n",
        "\n",
        "print('done instantiating the dataset class')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28)\n",
            "(10000, 28, 28, 1)\n",
            "done instantiating the dataset class\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFfzWGTEl2fI"
      },
      "source": [
        "# ***Le générateur***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uywJ-IBvRU-"
      },
      "source": [
        "#the Generator\n",
        "def build_generator(z_dim):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(256*7*7, input_dim=z_dim))\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "    \n",
        "    # 7*7*256 -> 14*14*256\n",
        "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    # 14*14*128 -> 14*14*64\n",
        "    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    # 14*14*64 => 28*28*1\n",
        "    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    model.add(Activation('tanh'))\n",
        "    \n",
        "    z = Input(shape=(z_dim,))\n",
        "    img = model(z)\n",
        "    \n",
        "    return Model(z, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDbhQ1hUl-mu"
      },
      "source": [
        "### ***Le discriminateur de base ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7De1JpGuv-kx"
      },
      "source": [
        "#The Core Discriminator\n",
        "def build_discriminator(img_shape):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # 28*28*1 => 14*14*32\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    # 14*14*32 => 7*7*64\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    # 7*7*64 => 3*3*128\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    model.add(Dropout(rate=0.5))\n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dense(num_classes))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OxX74LnmPjn"
      },
      "source": [
        "# ***Le discriminateur supervisé***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFyRTmp78ltT"
      },
      "source": [
        "def build_discriminator_supervised(discriminator_net):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(discriminator_net)\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up13Iml7mWj1"
      },
      "source": [
        "# ***Le discriminateur non supervisé***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y0sLM3N-hsv"
      },
      "source": [
        "def build_discriminator_unsupervised(discriminator_net):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(discriminator_net)\n",
        "    \n",
        "    def predict(x):\n",
        "        prediction = 1.0 - (1.0 / (K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\n",
        "        \n",
        "        return prediction\n",
        "    \n",
        "    model.add(Lambda(predict))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXVUVpD5mgcl"
      },
      "source": [
        "# ***Construction et compilation du modèle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1mioluj-m5k"
      },
      "source": [
        "#Building and Compiling the Model\n",
        "disc = build_discriminator(img_shape)\n",
        "\n",
        "sup_disc = build_discriminator_supervised(disc)\n",
        "sup_disc.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam())\n",
        "\n",
        "unsup_disc = build_discriminator_unsupervised(disc)\n",
        "unsup_disc.trainable = False\n",
        "unsup_disc.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam())\n",
        "\n",
        "gen = build_generator(z_dim)\n",
        "\n",
        "def combined(generator, discriminator):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    \n",
        "    return model\n",
        "\n",
        "sgan = combined(gen, unsup_disc)\n",
        "sgan.compile(loss='binary_crossentropy', optimizer=Adam())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "493PlVFImmkx"
      },
      "source": [
        "# ***Training***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfEWp64T-1gL"
      },
      "source": [
        "d_accuracies = []\n",
        "d_losses = []\n",
        "\n",
        "def train(iterations, batch_size, sample_interval):\n",
        "    real = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "    \n",
        "    for iteration in range(iterations):\n",
        "        imgs, labels = dataset.batch_labeled(batch_size)\n",
        "        labels = to_categorical(labels, num_classes=num_classes)\n",
        "        \n",
        "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "        \n",
        "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "        gen_imgs = gen.predict(z)\n",
        "        \n",
        "        d_loss_supervised, accuracy = sup_disc.train_on_batch(imgs, labels)\n",
        "        d_loss_real = unsup_disc.train_on_batch(imgs_unlabeled, real)\n",
        "        d_loss_fake = unsup_disc.train_on_batch(gen_imgs, fake)\n",
        "        \n",
        "        d_loss_unsupervised = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "        \n",
        "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "        gen_imgs = gen.predict(z)\n",
        "        \n",
        "        g_loss = sgan.train_on_batch(z, real)\n",
        "        \n",
        "        d_losses.append(d_loss_supervised)\n",
        "        d_accuracies.append(accuracy)\n",
        "        \n",
        "        if iteration % sample_interval == 0:\n",
        "            print('{} [D loss supervised: {:.4f}, acc: {:.2f}]'.format(iteration, d_loss_supervised, 100 * accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaKXsngv_ALy",
        "outputId": "c065c74a-b7b7-4c4d-8500-e07ea2a4f35f"
      },
      "source": [
        "iterations = 8000\n",
        "batch_size = 32\n",
        "sample_interval = 800\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [D loss supervised: 3.0591, acc: 12.50]\n",
            "800 [D loss supervised: 0.0001, acc: 100.00]\n",
            "1600 [D loss supervised: 0.0001, acc: 100.00]\n",
            "2400 [D loss supervised: 0.0000, acc: 100.00]\n",
            "3200 [D loss supervised: 0.0000, acc: 100.00]\n",
            "4000 [D loss supervised: 0.0000, acc: 100.00]\n",
            "4800 [D loss supervised: 0.0000, acc: 100.00]\n",
            "5600 [D loss supervised: 0.0000, acc: 100.00]\n",
            "6400 [D loss supervised: 0.0000, acc: 100.00]\n",
            "7200 [D loss supervised: 0.0000, acc: 100.00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlfSJ_2Um8dK"
      },
      "source": [
        "# **Évaluation du discriminateurs supervisée**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DYNVyh9_GJ-",
        "outputId": "c5deb438-270f-4b8c-95e3-b4535277592b"
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "y = to_categorical(y, num_classes)\n",
        "\n",
        "_, accuracy = sup_disc.evaluate(x, y)\n",
        "print('Test accuracty = {:.2f}%'.format(accuracy * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 1.5037 - accuracy: 0.6962\n",
            "Test accuracty = 69.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCBZt6xCnMIg"
      },
      "source": [
        "# ***Comparaison avec un classificateur entier ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmmeF8id_MI6",
        "outputId": "52562514-deb6-4281-a137-f134f9c734d3"
      },
      "source": [
        "base_disc = build_discriminator(img_shape)\n",
        "mnist_classifier = build_discriminator_supervised(base_disc)\n",
        "mnist_classifier.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam())\n",
        "for i in range(100):\n",
        "    x, y = dataset.batch_labeled(batch_size)\n",
        "    y = to_categorical(y, num_classes=num_classes)\n",
        "    sup_loss, sup_acc = mnist_classifier.train_on_batch(x, y)\n",
        "    \n",
        "    if i % 20 == 0:\n",
        "        print('iteration = {} / loss = {:.4f} / accuracy = {:.2f}'.format(i, sup_loss, sup_acc * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration = 0 / loss = 2.7016 / accuracy = 18.75\n",
            "iteration = 20 / loss = 0.1631 / accuracy = 93.75\n",
            "iteration = 40 / loss = 0.0223 / accuracy = 100.00\n",
            "iteration = 60 / loss = 0.0083 / accuracy = 100.00\n",
            "iteration = 80 / loss = 0.0133 / accuracy = 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwziscf2niZM"
      },
      "source": [
        "# ***##Evaluation du classificateur entier***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GBqGUyl_TN_",
        "outputId": "2a632069-f592-42fa-8a14-6b2e3b882f71"
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "y = to_categorical(y, num_classes)\n",
        "\n",
        "_, accuracy = mnist_classifier.evaluate(x, y)\n",
        "print('Test accuracty = {:.2f}%'.format(accuracy * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 1.9631 - accuracy: 0.5313\n",
            "Test accuracty = 53.13%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}